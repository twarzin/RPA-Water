rm(list = ls())  # clears memory
# Set working directory to same location of the R file location
base.dir <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(base.dir)
library(tidyr)
library(tibble)
library(ggplot2)
library(reshape2)
library(dplyr)  # Has the pipe operator %>%.
library(data.table)
# library(openxlsx)
library(readxl)
# Notes for faster functions
'
1) "data.table::fread" is faster than "read.csv" but I use "%>% as.data.frame" at the end
fread doesnot change the column names even if they are not allowed,
so I will limit its use here not to distribe the original code
2) "data.table::fwrite" is faster than "write.csv" and by default does not include the row names
3) "dplyr::inner_join" is quivalent but faster than "merge" , other options are full_join and left_join
4) "dplyr::arrange" is faster than attaching and using the order function
5) I replaced the for loop with a different login that runs in 3 min now instsead of 2 hr
'
usgs_ps <- read.csv("data/PS_HUC12_SW_2000_2020")
usgs_ps <- read.csv("data/PS_HUC12_SW_2000_2020.csv")
View(usgs_ps)
# Stack columns
stacked_ps <- pivot_longer(usgs_ps, cols = everything(), names_to = "Column", values_to = "Value")
View(stacked_ps)
View(stacked_ps)
View(stacked_ps)
rm(list = ls())  # clears memory
# for Travis:
setwd("D:/5_RPA/Demand model")
# Set working directory to same location of the R file location
base.dir <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(base.dir)
library(tidyr)
library(tibble)
library(ggplot2)
library(reshape2)
library(dplyr)  # Has the pipe operator %>%.
library(data.table)
# library(openxlsx)
library(readxl)
# Notes for faster functions
'
1) "data.table::fread" is faster than "read.csv" but I use "%>% as.data.frame" at the end
fread doesnot change the column names even if they are not allowed,
so I will limit its use here not to distribe the original code
2) "data.table::fwrite" is faster than "write.csv" and by default does not include the row names
3) "dplyr::inner_join" is quivalent but faster than "merge" , other options are full_join and left_join
4) "dplyr::arrange" is faster than attaching and using the order function
5) I replaced the for loop with a different login that runs in 3 min now instsead of 2 hr
'
# USGS moved to annual HUC12 water use data. This is for public supply:
usgs_ps <- read.csv("data/PS_HUC12_SW_2000_2020.csv")
stacked_ps <- cbind(usgs_ps[1:2], stack(usgs_ps[3:87]))
View(stacked_ps)
# Using reshape2
library(reshape2)
melted_ps <- melt(
usgs_ps, id.vars = c("year","month"),
measure.vars = c(colnames(usgs_ps)[3:22]))
# Using data.table
library(data.table)
melted_ps <- melt(
usgs_ps, id.vars = c("year","month"),
measure.vars = patterns("X"),
variable.name = "HUC", value.name = "PS")
ps <- as.data.table(usgs_ps)
melted_ps <- melt(
usgs_ps, id.vars = c("year","month"),
measure.vars = patterns("X"),
variable.name = "HUC", value.name = "PS")
rm(list = ls())  # clears memory
# Set working directory to same location of the R file location
base.dir <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(base.dir)
library(tidyr)
library(tibble)
library(ggplot2)
#library(reshape2)
library(dplyr)  # Has the pipe operator %>%.
library(data.table)
# library(openxlsx)
library(readxl)
# USGS moved to annual HUC12 water use data. This is for public supply:
usgs_ps <- read.csv("data/PS_HUC12_SW_2000_2020.csv")
# Using data.table
ps <- as.data.table(usgs_ps)
melted_ps <- melt(
usgs_ps, id.vars = c("year","month"),
measure.vars = patterns("X"),
variable.name = "HUC", value.name = "PS")
detach("package:reshape2", unload = TRUE)
melted_ps <- melt(
usgs_ps, id.vars = c("year","month"),
measure.vars = patterns("X"),
variable.name = "HUC", value.name = "PS")
melted_ps <- data.table::melt(
usgs_ps, id.vars = c("year","month"),
measure.vars = patterns("X"),
variable.name = "HUC", value.name = "PS")
stacked_ps <- cbind(usgs_ps[1:2], stack(usgs_ps[3:87022]))
View(stacked_ps)
stacked_ps <- stacked_ps %>% rename(public = values, huc12 = ind)
# Annual data
ps_annual <- stacked_ps %>%
group_by(year) %>%
summarise(public = sum(public, na.rm=TRUE))
# Annual data
ps_annual <- stacked_ps %>%
group_by(Year) %>%
summarise(public = sum(public, na.rm=TRUE))
View(ps_annual)
# Annual data
ps_annual <- stacked_ps %>%
group_by(Year, huc12) %>%
summarise(public = sum(public, na.rm=TRUE))
# Annual data
ps_annual <- stacked_ps %>%
group_by(huc12, Year) %>%
summarise(public = sum(public, na.rm=TRUE))
View(ps_annual)
ps_annual %>%
summarise(diff = last(public) - first(public), .by = c(huc12, year))
# Annual data
ps_annual <- stacked_ps %>%
group_by(huc12, Year) %>%
summarise(public = sum(public, na.rm=TRUE)),
# Annual data
ps_annual <- stacked_ps %>%
group_by(huc12, Year) %>%
summarise(public = sum(public, na.rm=TRUE))
View(ps_annual)
# Annual data
ps_annual <- stacked_ps %>%
group_by(huc12, Year) %>%
summarise(public = sum(public, na.rm=TRUE)),
ps_annual %>%
summarise(diff = last(public) - first(public), .by = c(huc12, year))
ps_annual %>%
summarise(diff = last(public) - first(public))
ps_annual <- ps_annual %>%
summarise(diff = last(public) - first(public))
ps.change <- ps_annual %>%
summarise(diff = (last(public) - first(public))/first(public))
# Annual data
ps_annual <- stacked_ps %>%
group_by(huc12, Year) %>%
summarise(public = sum(public, na.rm=TRUE))
ps.change <- ps_annual %>%
summarise(diff = (last(public) - first(public))/first(public))
View(ps.change)
View(ps.change)
library(sf)
install.packages(c("sf", "ggplot2", "dataRetrieval"))
install.packages(c("sf", "ggplot2", "dataRetrieval"))
library(sf)
library(dataRetrieval)
wd.1985 <- read_excel("1_BaseData/USGS raw water use/us85co.xls")
# Retrieve HUC shapefile for a specific region
huc_data <- readWBD("HUC12")  # Replace "HUC12" with the desired HUC level
